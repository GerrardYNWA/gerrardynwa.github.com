title: 视觉SLAM十四讲——非线性优化
date: 2017-09-20 22:27:49
categories: Algorithm

tags: [computer vision, slam]
---

　　重新整理学习《视觉SLAM十四讲》：非线性优化
<!-- more -->

## 1. 状态估计问题

- 最大后验估计（MAP）
- 最大似然估计（MLE）
- 原问题的最大化，相当于负对数最小化
- 把状态最大似然估计变成了最小二乘问题
- 直观解释
- 问题结构
- 如何求解？

## 2. 非线性最小二乘

- 迭代方式求解
- 确定增量的方式（即梯度下降策略）：一阶的或二阶的
- 泰勒展开
- 只保留一阶梯度（最速下降法，过于贪婪，zigzag问题）
- 保留二阶梯度（牛顿法，计算复杂的Hessian矩阵）
- 回避Hessian矩阵的计算
  - Gauss-Newton
  - Levenberg-Marquadt（阻尼牛顿法）
  - LM相比于GN，能够保证增量方程的正定性（认为近似只在一定范围内成立，如果近似不好则缩小范围）
  - 从增量方程上来看，可以看成一阶和二阶的混合（参数$\lambda$控制着两边的权重）
- 小结
  - 主要方法：最速下降、牛顿、GN、LM、DogLeg
  - 与线性规划不同，非线性需要针对具体问题具体分析
  - 问题非凸时，对初值敏感，会陷入局部最优
    - 目前没有非凸问题的通用最优值的寻找办法
    - 问题凸时，二阶方法通常一两步就能收敛

## 3. 面试题

### 1. 为什么SLAM中常用LM算法而不是高斯牛顿求解优化问题？
### 2. BA求解的三种方法（最速下降法，GN,LM)
### 3. 请写出L-M(Levenberg-Marquard)算法的流程并指出其相对于GN优化算法的优缺点。
### 4. GN和LM的迭代过程及优劣
### 5. 牛顿法、GN、LM的区别
### 6. 说一下Dog-Leg算法
### 7. 解释一下Gauss-Netwon和LM算法

